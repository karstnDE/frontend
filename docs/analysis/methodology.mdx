---
title: Data Pipeline & Methodology
---

# Data Pipeline & Methodology

This page documents the general approach to analyzing Solana DeFi protocols through on-chain data. For technology stack details, see [Technical Foundation](../../intro/tech-setup).

## Overview

The analysis process transforms raw blockchain transactions into structured insights:

1. **Data Collection** - Monitor relevant on-chain accounts and fetch transaction history
2. **Classification** - Identify what each transaction represents
3. **Attribution** - Organize transactions by relevant dimensions (token, type, pool, wallet)
4. **Aggregation** - Calculate totals, trends, and patterns
5. **Visualization** - Present findings through interactive charts

All processing happens offline. The site displays pre-computed results, ensuring fast performance.

---

## Data Collection

### What Gets Monitored

Analysis focuses on accounts relevant to understanding protocol operations:
- **Program accounts** - Smart contract interactions revealing protocol activity
- **Wallet addresses** - Key addresses involved in protocol operations
- **Token accounts** - Accounts holding protocol-relevant tokens

The specific accounts monitored depend on the analysis objective. Revenue analysis tracks treasury addresses. Staking analysis monitors staking program interactions. Position analysis follows liquidity pool accounts.

### Data Sources

**Primary:** Helius API provides enhanced transaction data with parsed logs and token transfers, eliminating complex instruction parsing.

**Supplemental:** Protocol websites and documentation provide context like token names and pool information.

### Update Cadence

Data updates run daily, appending new transactions while preserving historical consistency. When analytical methods improve, the entire dataset is reprocessed to maintain uniform standards.

---

## Transaction Classification

Understanding what each transaction represents is fundamental to analysis. Classification examines transaction logs to identify transaction purposes.

### Log-Based Classification

Solana programs emit log messages during execution. These logs contain instruction names describing operations. I extract these names and map them to transaction categories based on a predefined configuration.

For example:
- A log indicating "CollectFees" might represent fee collection
- A log indicating "Stake" represents a staking operation
- Different protocols use different instruction names for similar operations

### Classification Configuration

A configuration file maintains the mapping between instruction names and their meanings. This includes:
- Display names for user-facing labels
- Categorization for grouping related transactions
- Revenue flags indicating which transactions represent value generation
- Color schemes for consistent visualization

When protocols introduce new transaction types, the configuration is updated and historical data is reprocessed for consistency.

---

## Multi-Dimensional Attribution

Transactions are attributed across multiple dimensions simultaneously, enabling flexible analysis from different perspectives.

### Attribution Dimensions

Common attribution dimensions include:
- **Token** - Which asset was involved (SOL, USDC, protocol tokens)
- **Transaction Type** - What operation occurred (fees, swaps, stakes)
- **Pool** - Which liquidity pool or program was involved
- **Wallet** - Which addresses participated
- **Time** - When the transaction occurred

Each transaction receives labels for all applicable dimensions, allowing the same data to power different analytical views.

### Pool Identification

Pool attribution matches transaction accounts against a registry of known pool addresses. When explicit matches aren't found, token pair analysis can sometimes infer the pool based on the specific tokens involved.

The pool registry combines manually curated entries with auto-discovered pools, regularly updated as new pools are encountered.

---

## Token Value Normalization

To compare and aggregate transactions involving different tokens, values are normalized to a common denomination (typically SOL for Solana protocols).

### Conversion Approach

**On-chain rates:** When tokens are swapped, the actual exchange rate provides precise valuation for that moment.

**Same-day pricing:** Conversion rates apply only to transactions from the same time period, preventing retroactive adjustments and maintaining historical accuracy.

**Native token handling:** Wrapped versions of native tokens (like WSOL) convert 1:1 with their native counterparts.

### Pending Conversions

Tokens not yet converted remain tracked but excluded from normalized totals until conversion occurs. This ensures reported values reflect realized amounts rather than estimates.

---

## Data Quality Standards

### MECE Principle

Analysis follows the **Mutually Exclusive, Collectively Exhaustive** principle:
- **Mutually Exclusive** - Each transaction classified exactly once, preventing double-counting
- **Collectively Exhaustive** - All transactions accounted for, preventing gaps

### Verification Methods

Quality is maintained through:
- **Cross-validation** - Multiple calculation methods must agree
- **Completeness checks** - All transactions must be classified and attributed
- **Consistency validation** - Reprocessing historical data produces identical results
- **Benchmark comparison** - Where possible, totals verified against known values

### Accuracy Standard

Even small discrepancies (0.1 SOL) indicate analytical gaps requiring investigation. Attribution must be complete before publishing results.

---

## Protocol-Specific Analysis

Each protocol requires tailored analytical approaches based on its unique characteristics and available data. Protocol-specific methodology pages document these customizations.

### Current Coverage

- **[DefiTuna Methodology](./defituna/methodology)** - Treasury revenue attribution, staking analytics, usage tracking

Additional protocols will be added with similar depth and rigor.

---

## Frontend Presentation

Processed data exports as JSON files served as static assets. The frontend loads these files and renders interactive visualizations using Plotly charts.

A manifest timestamp indicates data freshness, displayed in the site header.

Static architecture ensures consistent performance regardless of data volume and makes the complete dataset accessible for download.

---

*This represents the general analytical framework. Protocol-specific pages detail particular implementations and unique challenges.*
