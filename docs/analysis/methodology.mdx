---
title: Data Pipeline & Methodology
---

# Data Pipeline & Methodology

This page documents the general approach to analyzing Solana DeFi protocols through on-chain data. For technology stack details, see [Technical Foundation](../../intro/tech-setup). For more details around individual DeFi protocols, check the respective section/ page.

## Overview

The end-to-end process transforms raw blockchain transactions into structured data and charts:

1. **Data Collection** - Monitor relevant on-chain accounts and fetch transaction history
2. **Classification** - Identify what each transaction represents
3. **Attribution** - Organize transactions by relevant dimensions (token, type, pool, wallet)
4. **Aggregation** - Calculate totals, trends, and patterns
5. **Visualization** - Present findings through interactive charts

All processing happens offline in another (private) backend repository. The site displays pre-computed results, ensuring fast performance.
Below you find more details for each of the five points mentioned above.

---

## Data Collection

### What Gets Monitored

Analysis focuses on accounts relevant to understanding protocol operations:
- **Program accounts** - Smart contract interactions revealing protocol activity, e.g. Treasury accounts
- **Wallet addresses** - Key addresses involved in protocol operations
- **Token accounts** - Accounts holding protocol-relevant tokens (ATAs)

The specific accounts monitored depend on the analysis objective. Revenue analysis tracks treasury addresses. Staking analysis monitors staking program interactions. Position analysis follows liquidity pool accounts.

### Data Sources

**Primary:** Helius API provides enhanced transaction data with parsed logs and token transfers, eliminating complex instruction parsing. Fallback APIs with free plans but heavy rate limiting are in place.

**Supplemental:** Protocol websites and documentation provide context like token names and pool information.

### Update Cadence

Data updates usually run daily via the automated GitHub Actions workflow (.github/workflows/daily-data.yml), When analytical methods improve, the entire dataset is reprocessed to maintain uniform standards.

---

## Transaction Classification

Understanding what each transaction represents is fundamental to analysis. Classification examines transaction logs to identify transaction purposes. **The core objective is identifying which Instruction is being executed** from the Program/Account we are observing.

### Log-Based Classification

**Solana programs themselves emit log messages during execution** as part of the blockchain's native logging system. These logs are not added by Helius, Solscan, or other data providers—they are produced directly by the on-chain programs and included in the transaction results returned by RPC nodes.

These logs contain instruction names describing operations. The classification system:
1. **Extracts instruction names** from the program logs
2. **Maps instruction names** to transaction categories based on a predefined configuration
3. ***Optional (protocol-specific)***: For certain analyses (e.g., staking loyalty) additional decoders parse Anchor IDLs to extract account-level context, but the core classification remains log-based for determinism

**Why Instructions Matter:** Instructions are the atomic operations in Solana programs. Understanding them is critical because:
- They reveal the exact purpose of each transaction (swap, stake, fee collection, etc.)
- Different protocols use different instruction names for similar operations
- IDL decoding provides structured information about parameters and accounts involved
- Accurate instruction identification enables precise revenue attribution and analytics

For example:
- A log containing "Instruction: CollectFees" represents fee collection
- A log containing "Instruction: Stake" represents a staking operation
- IDL decoding reveals which accounts received fees or which pools were involved

### Classification Configuration

A configuration file maintains the mapping between instruction names and their meanings. This includes:
- Display names for user-facing labels
- Categorization for grouping related transactions
- Revenue flags indicating which transactions represent value generation
- Color schemes for consistent visualization

When protocols introduce new transaction types, the configuration is updated and historical data is reprocessed for consistency.

---

## Multi-Dimensional Attribution

Transactions are attributed across multiple dimensions simultaneously, enabling flexible analysis from different perspectives.

### Attribution Dimensions

Common attribution dimensions include:
- **Token** - Which asset was involved (SOL, USDC, protocol tokens)
- **Transaction Type** - What operation occurred (fees, swaps, stakes)
- **Pool** - Which liquidity pool or program was involved
- **Wallet** - Which addresses participated
- **Time** - When the transaction occurred

Each transaction receives labels for all applicable dimensions, allowing the same data to power different analytical views.

### Pool Identification

**Pool identification works automatically** through account-based pattern matching. The system:

1. **Matches transaction accounts** against a registry of known pool addresses
2. **Auto-discovers new pools** by analyzing account structures and token pair patterns in transactions
3. **Infers pools from token pairs** when explicit address matches aren't found (e.g., a SOL-USDC swap likely involves the SOL-USDC pool)

The pool registry continuously expands as new pools are encountered, combining auto-discovered entries with manually verified addresses. No manual intervention is required for common pool operations—new pools are identified and tracked automatically during transaction processing.

---

## Token Value Normalization

To compare and aggregate transactions involving different tokens, values are normalized to a common denomination (typically SOL for Solana protocols).

### Conversion Approach

**On-chain rates:** When tokens are swapped, the actual exchange rate provides precise valuation for that moment.

**Same-day pricing:** Conversion rates are applied only to transactions occurring on the same day, ensuring that historical values are never retroactively adjusted based on future price changes.

*Example:* If the treasury receives 100 USDC on Day 1 and swaps it for 0.5 SOL on Day 5, the Day 1 revenue is recorded as 100 USDC (unconverted). On Day 5, the swap reveals the rate (100 USDC = 0.5 SOL), and Day 5's attribution includes both the swap and the converted value. Day 1's historical record remains unchanged as 100 USDC, preserving the actual state of the treasury on that day. This prevents distortions where today's prices misrepresent historical revenue.

**Native token handling:** Wrapped versions of native tokens (like WSOL) convert 1:1 with their native counterparts.

### Pending Conversions

**Context:** This is particularly relevant for **treasury accounts** in DeFi protocols. Treasuries often accumulate diverse tokens (fee revenue in USDC, BONK, various LP tokens, etc.), but **only a specific token is made available for distribution to stakers** (typically the protocol's native token or SOL).

For example, a treasury might receive:
- 100 USDC from swap fees
- 500 BONK from referral rewards
- 10 LP tokens from liquidity provision

But the protocol only distributes SOL to stakers. Until the treasury swaps these tokens for SOL, they remain "pending conversions."

**Accounting Treatment:** Tokens not yet converted remain tracked in their native denomination but excluded from SOL-normalized totals until conversion occurs on-chain. This ensures reported values reflect **realized amounts** (actual conversions that happened) rather than estimates based on external price feeds. When the treasury eventually swaps 100 USDC for 0.5 SOL, that 0.5 SOL is recorded as revenue on the day of the swap, not on the day the USDC was originally received.

---

## Data Quality Standards

Data Quality is paramount for a project like this. I spend a lot of time on ensuring that the numbers are exact and make sense. There are different ways to make sure the numbers are correct.

### Verification Methods

Quality is maintained through:
- **Cross-validation** - Multiple calculation methods must agree
- **Completeness checks** - All transactions must be classified and attributed
- **Consistency validation** - Reprocessing historical data produces identical results
- **Benchmark comparison** - Where possible, totals verified against known values

### Accuracy Standard

Even small discrepancies (0.1 SOL) indicate analytical gaps requiring investigation. This might seem like minor, but if the root cause is not addressed, a small gap can grow to a big gap and indiates the situation is not fully understood.

---

## Protocol-Specific Analysis

Each protocol requires tailored analytical approaches based on its unique characteristics and available data. Protocol-specific methodology pages document these customizations.

### Current Coverage

- **[DefiTuna Methodology](./defituna/methodology)** - Treasury revenue attribution, staking analytics, usage tracking

Additional protocols will be added with similar depth and rigor.

---

## Frontend Presentation

Processed data exports as JSON files served as static assets. The frontend loads these files and renders interactive visualizations using Plotly charts.

A manifest timestamp indicates data freshness, displayed in the site header.

Static architecture ensures consistent performance regardless of data volume and makes the complete dataset accessible for download.

---

*This represents the general analytical framework. Protocol-specific pages detail particular implementations and unique challenges.*
